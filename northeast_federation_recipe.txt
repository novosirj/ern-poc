The cast:

ern.hpc.rutgers.edu - slurm database, xrootd redirector, ldap server, ganglia web, perfsonar web
khan.hpc.rutgers.edu - xrootd data server

mace - 3-node physical cluster at Rutgers
mace - login node, scheduler, local nfs
mace[1-2] - compute nodes

~/fedconfig - site-specific config files - generate or get from Rutgers and modify

OS: Centos 7.4
OpenHPC recipe: 1.3.4
Scheduler: slurm 17.11
Filesystem: xrootdfs with local nfs export
Authz, authn: ldap, replace with shibboleth
Other: ganglia, perfsonar
Eval: ceph, openafs, stashcache, kubernetes, singularity

Issues:
xrootd won’t mount until ldap is ready
xrootd security works from Rutgers but not MGHPCC, temporarily disabled


# bare metal install
# install centos 7.4 minimal
# set up ssh keys from local management servers
yum -y update
# this one fails - did update do it already?  just skip it and install the other groups.
#yum -y groupinstall “Infrastructure Server”
yum -y groupinstall "Hardware Monitoring Utilities" "Performance Tools" "Development Tools" "Network File System Client" "Console Internet Tools" "Networking Tools" "System Administration Tools" "System Management" “Compatibility Libraries” “Platform Development”
# customize the “local” part of hosts.allow and hosts to add management, monitoring, etc.
# y to overwrite prompt
# skip these three for the compute nodes - do it once keys are set up from mace
cp ~/fedconfig/hosts /etc
cp ~/fedconfig/hosts.allow /etc
cp ~/fedconfig/hosts.deny /etc
yum -y install http://build.openhpc.community/OpenHPC:/1.3/CentOS_7/x86_64/ohpc-release-1.3-1.el7.x86_64.rpm
yum -y update
yum -y install ohpc-base
sed -i s/SELINUX=enforcing/SELINUX=permissive/ /etc/selinux/config
setenforce 0
yum -y install nmap iptraf-ng iperf3 hdparm msr-tools mlocate trafshow yum-utils
# customize


# mace
# follow bare metal install for mace and mace1-5

# firewall
firewall-cmd --list-all-zones
firewall-cmd --zone=public --remove-service=dhcpv6-client
firewall-cmd --zone=public --remove-service=ssh
firewall-cmd --info-zone=public
# add internal interface to trusted zone
# ZONE=trusted
vi /etc/sysconfig/network-scripts/ifcfg-enp6s4f0
# these are Rutgers-local networks - replace with your campus management network
firewall-cmd --zone=public --add-rich-rule='rule family="ipv4" source address="172.16.94.0/24" accept'
firewall-cmd --zone=public --add-rich-rule='rule family="ipv4" source address="172.16.74.64/26" accept'
firewall-cmd --zone=public --add-masquerade
firewall-cmd --runtime-to-permanent

# node ssh keys
ssh-keygen
ssh-copy-id mace1
ssh-copy-id mace2
ssh-copy-id mace3
ssh-copy-id mace4
ssh-copy-id mace5
pdsh -w mace[1-5] "uptime"
echo "net.ipv4.ip_forward = 1" >> /etc/sysctl.conf
sysctl -p /etc/sysctl.conf

# ldap
yum install sssd openldap-clients
authconfig --updateall --enableldap --enableldapauth --ldapserver=ldap://ern.hpc.rutgers.edu:389 --ldapbasedn=dc=ufp,dc=hpc,dc=rutgers,dc=edu --enableldaptls --enableldapstarttls
cp ~/fedconfig/ern_hpc_rutgers_edu_interm.cer /etc/openldap/cacerts
cacertdir_rehash /etc/openldap/cacerts/
systemctl restart sssd
id babbott

# create home dirs
yum install oddjob-mkhomedir
systemctl enable oddjobd
systemctl start oddjobd
authconfig --enablemkhomedir --update

# slurm, munge
# check that user are already in ldap
id slurm
id munge
yum install ohpc-slurm-server ohpc-slurm-client
cp ~/fedconfig/munge.key /etc/munge/munge.key
systemctl restart munge
# Modify /etc/slurm/slurm.conf with the below
#ClusterName=mace
#ControlMachine=mace.local
#AccountingStorageType=accounting_storage/slurmdbd
#AccountingStorageHost=ern.hpc.rutgers.edu
#NodeName=mace[1-5] Sockets=2 CoresPerSocket=4 ThreadsPerCore=1 State=UNKNOWN
#PartitionName=DEFAULT DefaultTime=1:00:00 MaxTime=3-0 State=UP
#PartitionName=mace Nodes=mace[1-5] Default=YES
#PartitionName=ern Nodes=mace[1-5]
#FederationParameters=fed_display
vi /etc/slurm/slurm.conf
systemctl enable slurmctld
systemctl restart slurmctld
# notify Bill of the cluster name and scheduler ip address,
# wait for confirmation that the slurmdbd is modified
systemctl restart slurmctld
# send ssh keys
# and usernames to set up in ldap

# nfs - mace exports /home and /xrootd to compute nodes
#/home   192.168.0.0/255.255.0.0(rw,fsid=0)
vi /etc/exports
exportfs -r
exportfs
systemctl enable nfs-server
systemctl start nfs-server

# xrootd
# check that xrootd user is already in ldap
id xrootd
yum -y install xrootd-client xrootd-client-libs xrootd-fuse
mkdir /var/log/xrootd
chown -R xrootd:xrootd /var/log/xrootd/
cp ~/fedconfig/xrootd.key /etc/xrootd
chown -R xrootd:xrootd /etc/xrootd
#xrootdfs /xrootd fuse rdr=xroot://khan.hpc.rutgers.edu:1094//xdata/,uid=xrootd,sss=/etc/xrootd/xrootd.key,noauto 0 0
vi /etc/fstab
mkdir /xrootd
mount /xrootd
ls -al /xrootd
su - babbott
# only /xrootd/users/babbott should be writeable for babbott
# not currently working
cd /xrootd/users/babbott
touch test
# add to nfs
#/xrootd   192.168.0.0/255.255.0.0(rw,fsid=1)
vi /etc/exports
exportfs -r
exportfs

# add user ssh keys
su - babbott
ssh-keygen
cat .ssh/id_rsa.pub >> .ssh/authorized_keys
chmod 400 .ssh/authorized_keys
ssh mace “uptime”
exit

# perfsonar
yum install http://software.internet2.edu/rpms/el7/x86_64/main/RPMS/perfSONAR-repo-0.8-1.noarch.rpm
yum install perfSONAR-repo-staging
yum clean all
yum install perfsonar-testpoint
yum install perfsonar-toolkit-servicewatcher
yum install perfsonar-toolkit-sysctl
yum install perfsonar-toolkit-systemenv-testpoint
# switch to ren limits
cd /etc/pscheduler
mv limits.conf limits.conf-orig
cp /usr/share/doc/pscheduler/limit-examples/identifier-ip-cidr-list-url limits.conf
chown root:pscheduler limits.conf
pscheduler validate-limits
cp ~/fedconfig/perfsonar.xml /etc/firewalld/services/
firewall-cmd --reload
# get mesh config from ern.hpc
psconfig remote add --configure-archives "https://ern.hpc.rutgers.edu/psconfig/ernmesh.json"
# reboot the server and the services should start automatically

# ganglia
yum -y install ganglia-gmond-ohpc
# cluster name, udp send host
cp ~/fedconfig/gmond.conf /etc/ganglia
systemctl enable gmond
systemctl start gmond

# firewall settings
# all from local subnet - replace with yours
firewall-cmd --zone=public --add-rich-rule='rule family="ipv4" source address="128.6.226.160/27" accept'
# ssh, slurm, perfsonar from other member sites
firewall-cmd --zone=public --add-rich-rule='rule family="ipv4" source address="192.69.102.192/27" port port=22 protocol=tcp accept'
firewall-cmd --zone=public --add-rich-rule='rule family="ipv4" source address="192.69.102.192/27" port port=6817 protocol=tcp accept'
firewall-cmd --zone=public --add-rich-rule='rule family="ipv4" source address="192.69.102.192/27" service name=perfsonar accept'
# ganglia from central
firewall-cmd --zone=public --add-rich-rule='rule family="ipv4" source address="128.6.226.160/27" port port=8649 protocol=tcp accept'
firewall-cmd --runtime-to-permanent

# user job
yum install boinc-client boinc-client-doc


# complete mace1-5 (bare metal and mace keys already done)
# commands run from mace
for i in {1..5};do scp ~/fedconfig/hosts mace$i:/etc;done
for i in {1..5};do scp ~/fedconfig/hosts.allow mace$i:/etc;done
for i in {1..5};do scp ~/fedconfig/hosts.deny mace$i:/etc;done
pdsh -w mace[1-5] ‘systemctl disable firewalld’

# ldap
pdsh -w mace[1-5] 'yum -y install sssd openldap-clients'
pdsh -w mace[1-5] ‘mkdir /etc/openldap/cacerts’
for i in {1..5};do echo mace$i;scp ~/fedconfig/ern_hpc_rutgers_edu_interm.cer mace$i:/etc/openldap/cacerts;ssh mace$i ‘cacertdir_rehash /etc/openldap/cacerts/’;done
pdsh -w mace[1-5] 'authconfig --updateall --enableldap --enableldapauth --ldapserver=ldap://ern.hpc.rutgers.edu:389 --ldapbasedn=dc=ufp,dc=hpc,dc=rutgers,dc=edu --enableldaptls --enableldapstarttls'
pdsh -w mace[1-5] ‘systemctl restart sssd’
pdsh -w mace[1-5] ‘id babbott’

# slurm
# should be 20003, 20002
pdsh -w mace[1-5] ‘id slurm;id munge’
# might not need ohpc-slurm-client here, just server
pdsh -w mace[1-5] ‘yum -y install ohpc-slurm-client ohpc-slurm-server’
for i in {1..5};do scp ~/fedconfig/munge.key mace$i:/etc/munge/munge.key;done
pdsh -w mace[1-5] ‘systemctl restart munge’
for i in {1..5};do scp ~/fedconfig/slurm.conf mace$i:/etc/slurm;done
pdsh -w mace[1-5] “systemctl enable slurmd;systemctl restart slurmd”

# nfs mounts - do on each compute node
ssh mace1
# comment out local /home if present, add nfs home
#mace.local:/home /home                   nfs     defaults        0 0
vi /etc/fstab
exit
# repeat for mace2-5
# back to mace
pdsh -w mace[1-5] 'umount /home;mount /home'

# xrootd
# on mace1-5
ssh mace1
#mace.local:/xrootd /xrootd        nfs    defaults    0 0
vi /etc/fstab
exit
# back to mace
pdsh -w mace[1-5] ‘mkdir /xrootd;mount /xrootd;ls /xrootd’

# ganglia
pdsh -w mace[1-5] 'yum -y install ganglia-gmond-ohpc'
for i in {1..5};do echo mace$i;scp ~/fedconfig/gmond.conf mace$i:/etc/ganglia;done
pdsh -w mace[1-5] 'systemctl enable gmond'
pdsh -w mace[1-5] 'systemctl start gmond'

# confirm user can ssh from mace to mace1-5 without password
su - babbott
for i in {1..5};do ssh mace$i “uptime”;done
exit

# user job
pdsh -w mace[1-5] ‘yum -y install boinc-client boinc-client-doc’
