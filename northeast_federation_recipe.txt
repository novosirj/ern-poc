The cast:

ern.hpc.rutgers.edu - slurm database, xrootd redirector, ldap server, ganglia web, perfsonar web
khan.hpc.rutgers.edu - xrootd data server

mace - 3-node physical cluster at Rutgers
mace - login node, scheduler, local nfs
mace[1-2] - compute nodes

~/fedconfig - site-specific config files - generate or get from Rutgers and modify

OS: Centos 7.4
OpenHPC recipe: 1.3.4
Scheduler: slurm 17.11
Filesystem: xrootdfs with local nfs export
Authz, authn: ldap, replace with shibboleth
Other: ganglia, perfsonar, singularity
Eval: ceph, openafs, stashcache, kubernetes

Issues:
xrootd won’t mount until ldap is ready (test systemd fix)
xrootd security works from Rutgers but not external, temporarily disabled

# bare metal install
# install centos 7.4 minimal
# set up ssh keys from local management servers
# git clone ern repo
git clone https://github.com/rutgers-oarc/ern-poc.git
yum -y update
# this one fails - did update do it already?  just skip it and install the other groups.
#yum -y groupinstall “Infrastructure Server”
yum -y groupinstall "Hardware Monitoring Utilities" "Performance Tools" "Development Tools" "Network File System Client" "Console Internet Tools" "Networking Tools" "System Administration Tools" "System Management" “Compatibility Libraries” “Platform Development”
#
# create local versions and append
# skip these three for the compute nodes - do it once keys are set up from mace
cat ~/ern-poc/fedconfig/hosts >> /etc/hosts
cat ~/ern-poc/fedconfig/hosts.allow >> /etc/hosts.allow
cat ~/ern-poc/fedconfig/hosts.deny >> /etc/hosts.deny
#
yum -y install http://build.openhpc.community/OpenHPC:/1.3/CentOS_7/x86_64/ohpc-release-1.3-1.el7.x86_64.rpm
yum -y update
yum -y install ohpc-base
#
sed -i s/SELINUX=enforcing/SELINUX=permissive/ /etc/selinux/config
setenforce 0
yum -y install nmap iptraf-ng iperf3 hdparm msr-tools mlocate trafshow yum-utils
# customize


# mace
# follow bare metal install for mace and mace1-2

# firewall
firewall-cmd --list-all-zones
firewall-cmd --zone=public --remove-service=dhcpv6-client
firewall-cmd --info-zone=public
# add internal interface to trusted zone
# ZONE=trusted
vi /etc/sysconfig/network-scripts/ifcfg-enp6s4f0
# these are Rutgers-local networks - replace with your campus management network
firewall-cmd --zone=public --add-rich-rule='rule family="ipv4" source address="172.16.94.0/24" accept'
firewall-cmd --zone=public --add-rich-rule='rule family="ipv4" source address="172.16.74.64/26" accept'
firewall-cmd --zone=public --add-masquerade
firewall-cmd --zone=public --add-port=6817/tcp
cp ~/ern-poc/fedconfig/perfsonar.xml /etc/firewalld/services/
firewall-cmd --zone=public --add-service=perfsonar
firewall-cmd --runtime-to-permanent
firewall-cmd --reload

# this should leave you with ssh, perfsonar and 6817/tcp open in zone public.
# these will be restricted with hosts.allow, ps limits and munge keys.

# node ssh keys
ssh-keygen
ssh-copy-id mace1
ssh-copy-id mace2
pdsh -w mace[1-2] "uptime"
echo "net.ipv4.ip_forward = 1" >> /etc/sysctl.conf
sysctl -p /etc/sysctl.conf

# ldap
yum install sssd openldap-clients
authconfig --updateall --enableldap --enableldapauth --ldapserver=ldap://ern.hpc.rutgers.edu:389 --ldapbasedn=dc=ufp,dc=hpc,dc=rutgers,dc=edu --enableldaptls --enableldapstarttls
cp ~/ern-poc/fedconfig/ern_hpc_rutgers_edu_interm.cer /etc/openldap/cacerts
cacertdir_rehash /etc/openldap/cacerts/
systemctl restart sssd
id babbott

# create home dirs
yum install oddjob-mkhomedir
systemctl enable oddjobd
systemctl start oddjobd
authconfig --enablemkhomedir --update

# slurm, munge
# check that user slurm is already in ldap, munge probably not necessary
id slurm
yum install ohpc-slurm-server ohpc-slurm-client
# acquire munge.key, copy to /etc/munge
chown munge:munge /etc/munge/munge.key
chmod 400 /etc/munge/munge.key
systemctl restart munge
# Modify /etc/slurm/slurm.conf with the below, replacing mace with your name
#ClusterName=mace
#ControlMachine=mace.local
#AccountingStorageType=accounting_storage/slurmdbd
#AccountingStorageHost=ern.hpc.rutgers.edu
#NodeName=mace[1-2] Sockets=2 CoresPerSocket=4 ThreadsPerCore=1 State=UNKNOWN
#PartitionName=DEFAULT DefaultTime=1:00:00 MaxTime=3-0 State=UP
#PartitionName=mace Nodes=mace[1-2] Default=YES
#PartitionName=ern Nodes=mace[1-2]
#FederationParameters=fed_display
vi /etc/slurm/slurm.conf
systemctl enable slurmctld
systemctl restart slurmctld
# notify Bill of the cluster name and scheduler ip address,
# wait for confirmation that the slurmdbd is modified
systemctl restart slurmctld
# send ssh keys
# and usernames to set up in ldap

# nfs - mace exports /home and /xrootd/* to compute nodes, just do home now
#/home   192.168.0.0/255.255.0.0(rw,fsid=0)
vi /etc/exports
exportfs -r
exportfs
systemctl enable nfs-server
systemctl start nfs-server

# xrootd
# check that xrootd user is already in ldap
id xrootd
yum -y install xrootd-client xrootd-client-libs xrootd-fuse
chown -R xrootd:xrootd /var/log/xrootd/
cp ~/ern-poc/fedconfig/xrootd.key /etc/xrootd
chown -R xrootd:xrootd /etc/xrootd
#xrootdfs /xrootd/ern.rutgers fuse rdr=xroot://ern.hpc.rutgers.edu:1094//xdata/,uid=xrootd,noauto 0 0
#xrootdfs /xrootd/khan.rutgers fuse rdr=xroot://khan.hpc.rutgers.edu:1094//xdata/,uid=xrootd,noauto 0 0
vi /etc/fstab
mkdir /xrootd
mkdir /xrootd/ern.rutgers
mkdir /xrootd/khan.rutgers
mount /xrootd/ern.rutgers
# khan under construction for a bit
ls -al /xrootd/ern.rutgers
su - babbott
cd /xrootd/ern.rutgers/babbott
touch test
# add to nfs
#/xrootd/ern.rutgers   192.168.0.0/255.255.0.0(rw,fsid=11)
#/xrootd/khan.rutgers   192.168.0.0/255.255.0.0(rw,fsid=12)
vi /etc/exports
exportfs -r
exportfs

# set up user ssh keys - replace babbott with your username
su - babbott
ssh-keygen
cat .ssh/id_rsa.pub >> .ssh/authorized_keys
chmod 400 .ssh/authorized_keys
ssh mace “uptime”
exit

# perfsonar
yum install http://software.internet2.edu/rpms/el7/x86_64/main/RPMS/perfSONAR-repo-0.8-1.noarch.rpm
yum install perfSONAR-repo-staging
yum clean all
yum install perfsonar-testpoint
yum install perfsonar-toolkit-servicewatcher
yum install perfsonar-toolkit-sysctl
yum install perfsonar-toolkit-systemenv-testpoint
# switch to ren limits
cd /etc/pscheduler
mv limits.conf limits.conf-orig
cp /usr/share/doc/pscheduler/limit-examples/identifier-ip-cidr-list-url limits.conf
chown root:pscheduler limits.conf
pscheduler validate-limits
# get mesh config from ern.hpc and the northeast mesh
psconfig remote add --configure-archives "https://ern.hpc.rutgers.edu/psconfig/ernmesh.json"
psconfig remote add --configure-archives "https://mesh.hpc.rutgers.edu/psconfig/northeastmesh.json"
# reboot the server and the services should start automatically

# ganglia
yum -y install ganglia-gmond-ohpc
cp ~/ern-poc/fedconfig/gmond.conf /etc/ganglia
# cluster name, udp send host
vi /etc/ganglia/gmond.conf
systemctl enable gmond
systemctl start gmond

# firewall settings
# all from local subnet - replace with yours
firewall-cmd --zone=public --add-rich-rule='rule family="ipv4" source address="128.6.226.160/27" accept'
# ssh, slurm, perfsonar should already be in zone public
# ganglia from central
firewall-cmd --zone=public --add-rich-rule='rule family="ipv4" source address="128.6.226.160/27" port port=8649 protocol=tcp accept'
firewall-cmd --runtime-to-permanent

# user job
yum install boinc-client boinc-client-doc

# singularity
yum -y install singularity-ohpc



# complete mace1-2 (bare metal and mace keys already done)
# commands run from mace
for i in {1..2};do scp /etc/hosts mace$i:/etc;done
for i in {1..2};do scp /etc/hosts.allow mace$i:/etc;done
for i in {1..2};do scp /etc/hosts.deny mace$i:/etc;done
pdsh -w mace[1-2] ‘systemctl disable firewalld’

# ldap
pdsh -w mace[1-2] 'yum -y install sssd openldap-clients'
pdsh -w mace[1-2] ‘mkdir /etc/openldap/cacerts’
for i in {1..2};do echo mace$i;scp ~/ern-poc/fedconfig/ern_hpc_rutgers_edu_interm.cer mace$i:/etc/openldap/cacerts;ssh mace$i ‘cacertdir_rehash /etc/openldap/cacerts/’;done
pdsh -w mace[1-2] 'authconfig --updateall --enableldap --enableldapauth --ldapserver=ldap://ern.hpc.rutgers.edu:389 --ldapbasedn=dc=ufp,dc=hpc,dc=rutgers,dc=edu --enableldaptls --enableldapstarttls'
pdsh -w mace[1-2] ‘systemctl restart sssd’
pdsh -w mace[1-2] ‘id babbott’

# slurm
# should be 20003, 20002
pdsh -w mace[1-2] ‘id slurm;id munge’
# might not need ohpc-slurm-client here, just server
pdsh -w mace[1-2] ‘yum -y install ohpc-slurm-client ohpc-slurm-server’
for i in {1..5};do scp ~/fedconfig/munge.key mace$i:/etc/munge/munge.key;done
pdsh -w mace[1-2] ‘systemctl restart munge’
for i in {1..5};do scp ~/fedconfig/slurm.conf mace$i:/etc/slurm;done
pdsh -w mace[1-2] “systemctl enable slurmd;systemctl restart slurmd”

# nfs mounts - do on each compute node
ssh mace1
# comment out local /home if present, add nfs home
#mace.local:/home /home                   nfs     defaults        0 0
vi /etc/fstab
exit
# repeat for mace2
# back to mace
pdsh -w mace[1-2] 'umount /home;mount /home'

# xrootd
# on mace1-2
ssh mace1
#mace.local:/xrootd/ern.rutgers /xrootd/ern.rutgers        nfs    defaults    0 0
#mace.local:/xrootd/khan.rutgers /xrootd/khan.rutgers        nfs    defaults    0 0
vi /etc/fstab
exit
# back to mace
pdsh -w mace[1-2] ‘mkdir /xrootd;mkdir /xrootd/ern.rutgers;mkdir /xrootd/khan.rutgers;mount /xrootd/ern.rutgers;ls /xrootd;ls /xrootd/ern.rutgers’

# ganglia
pdsh -w mace[1-2] 'yum -y install ganglia-gmond-ohpc'
for i in {1..2};do echo mace$i;scp /etc/ganglia/gmond.conf mace$i:/etc/ganglia;done
pdsh -w mace[1-2] 'systemctl enable gmond'
pdsh -w mace[1-2] 'systemctl start gmond'

# confirm user can ssh from mace to mace1-2 without password
su - babbott
for i in {1..2};do ssh mace$i “uptime”;done
exit

# user job
pdsh -w mace[1-2] ‘yum -y install boinc-client boinc-client-doc’

# singularity
pdsh -w mace[1-2] 'yum -y install singularity-ohpc'
