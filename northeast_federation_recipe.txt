The cast:

ern.hpc.rutgers.edu - slurm database, ldap server, ganglia web, perfsonar web
khan.hpc.rutgers.edu - cephfs cluster

mace - 3-node physical cluster at Rutgers
mace - login node, scheduler, local nfs
mace[1-2] - compute nodes

~/fedconfig - site-specific config files - generate or get from Rutgers and modify

OS: Centos 7.4
OpenHPC recipe: 1.3.4
Scheduler: slurm 17.11
Filesystem: cephfs, exported via nfs to compute nodes
Authz, authn: ldap, replace with shibboleth
Other: ganglia, perfsonar, singularity
Eval: stashcache, kubernetes


# mace replaced with <scheduler> below
# mace.local replaced with <scheduler.local>
# mace1-2 replaced with <node1-2>
# follow bare metal install for <scheduler> and <node1-2>

---
# bare metal install
# install centos 7.4 minimal
# set up ssh keys from local management servers
# git clone ern repo
git clone https://github.com/rutgers-oarc/ern-poc.git
yum -y update
yum -y groupinstall "Hardware Monitoring Utilities" "Performance Tools" "Development Tools" "Network File System Client" "Console Internet Tools" "Networking Tools" "System Administration Tools" "System Management" “Compatibility Libraries” “Platform Development”
#
# create minimal hosts and hosts.allow - ansible will append later
# skip these three for the compute nodes - do it once keys are set up from mace
vi /etc/hosts
vi /etc/hosts.allow
vi /etc/hosts.deny

# add to hosts.allow for automation etc.
echo "Rutgers central ERN services" >> /etc/hosts.allow
echo "all: 128.6.226.160/255.255.255.224" >> /etc/hosts.allow
#
# and block other stuff
echo "all: all" >> /etc/hosts.deny

yum -y install http://build.openhpc.community/OpenHPC:/1.3/CentOS_7/x86_64/ohpc-release-1.3-1.el7.x86_64.rpm
yum -y update
yum -y install ohpc-base
#
sed -i s/SELINUX=enforcing/SELINUX=permissive/ /etc/selinux/config
setenforce 0
yum -y install nmap iptraf-ng iperf3 hdparm msr-tools mlocate trafshow yum-utils
# done with bare metal install

# on <scheduler>

# firewall
firewall-cmd --list-all-zones
firewall-cmd --zone=public --remove-service=dhcpv6-client
firewall-cmd --info-zone=public
# add internal interface to trusted zone
# ZONE=trusted
vi /etc/sysconfig/network-scripts/ifcfg-enp6s4f0
# these are Rutgers-local networks - replace with your campus management network
firewall-cmd --zone=public --add-rich-rule='rule family="ipv4" source address="a.b.c.d/e" accept'
firewall-cmd --zone=public --add-masquerade
firewall-cmd --zone=public --add-port=6817/tcp
cp ~/ern-poc/fedconfig/perfsonar.xml /etc/firewalld/services/
firewall-cmd --zone=public --add-service=perfsonar
firewall-cmd --runtime-to-permanent
firewall-cmd --reload

# this should leave you with ssh, perfsonar and 6817/tcp open in zone public.
# these will be restricted with hosts.allow, ps limits and munge keys.

# node ssh keys
ssh-keygen
ssh-copy-id <node1>
ssh-copy-id <node2>
pdsh -w <node>[1-2] "uptime"
echo "net.ipv4.ip_forward = 1" >> /etc/sysctl.conf
sysctl -p /etc/sysctl.conf

# ldap
yum install sssd openldap-clients
authconfig --updateall --enableldap --enableldapauth --ldapserver=ldap://ern.hpc.rutgers.edu:389 --ldapbasedn=dc=ufp,dc=hpc,dc=rutgers,dc=edu --enableldaptls --enableldapstarttls
cp ~/ern-poc/fedconfig/ern_hpc_rutgers_edu_interm.cer /etc/openldap/cacerts
cacertdir_rehash /etc/openldap/cacerts/
systemctl restart sssd
id babbott

# slurm, munge
# check that user slurm is already in ldap, munge probably not necessary
id slurm
yum install ohpc-slurm-server ohpc-slurm-client
# acquire munge.key, copy to /etc/munge
chown munge:munge /etc/munge/munge.key
chmod 400 /etc/munge/munge.key
systemctl restart munge
# Modify /etc/slurm/slurm.conf with the below, replacing mace with your name
#ClusterName=<somename>
#ControlMachine=<scheduler.local>
#AccountingStorageType=accounting_storage/slurmdbd
#AccountingStorageHost=ern.hpc.rutgers.edu
#NodeName=<node>[1-2] Sockets=2 CoresPerSocket=4 ThreadsPerCore=1 State=UNKNOWN
#PartitionName=DEFAULT DefaultTime=1:00:00 MaxTime=3-0 State=UP
#PartitionName=ern Nodes=<node>[1-2] Default=YES
#FederationParameters=fed_display
vi /etc/slurm/slurm.conf
systemctl enable slurmctld
systemctl restart slurmctld
# notify Bill of the cluster name and scheduler ip address,
# wait for confirmation that the slurmdbd is modified
systemctl restart slurmctld
# send ssh keys
# and usernames to set up in ldap

# cephfs
# first, unmount /home
rpm -Uhv http://download.ceph.com/rpm-mimic/el7/noarch/ceph-release-1-1.el7.noarch.rpm
yum clean all
yum update
yum install ceph-common
# get khan.key from Bill
cp khan.key /etc/ceph
chmod 400 /etc/ceph/khan.key
# add this line to /etc/fstab:
#khan.hpc.rutgers.edu:6789:/     /home   ceph    name=admin,secretfile=/etc/ceph/khan.key,_netdev,noatime        0 0
mount /home
ls /home

# add to nfs and export
#/home           192.168.0.0/255.255.0.0(rw,fsid=0)
vi /etc/exports
exportfs -r
exportfs
systemctl enable nfs-server
systemctl start nfs-server

# set up user ssh keys - replace babbott with your username
su - babbott
ssh-keygen
cat .ssh/id_rsa.pub >> .ssh/authorized_keys
chmod 400 .ssh/authorized_keys
ssh <scheduler> “uptime”
exit

# perfsonar
yum install http://software.internet2.edu/rpms/el7/x86_64/main/RPMS/perfSONAR-repo-0.8-1.noarch.rpm
yum install perfSONAR-repo-staging
yum clean all
yum install perfsonar-testpoint
yum install perfsonar-toolkit-servicewatcher
yum install perfsonar-toolkit-sysctl
yum install perfsonar-toolkit-systemenv-testpoint
# switch to ren limits
cd /etc/pscheduler
mv limits.conf limits.conf-orig
cp /usr/share/doc/pscheduler/limit-examples/identifier-ip-cidr-list-url limits.conf
chown root:pscheduler limits.conf
pscheduler validate-limits
# get mesh config from ern.hpc and the northeast mesh
psconfig remote add --configure-archives "https://ern.hpc.rutgers.edu/psconfig/ernmesh.json"
psconfig remote add --configure-archives "https://mesh.hpc.rutgers.edu/psconfig/northeastmesh.json"
# reboot the server and the services should start automatically

# ganglia
yum -y install ganglia-gmond-ohpc
cp ~/ern-poc/fedconfig/gmond.conf /etc/ganglia
# cluster name, udp send host
vi /etc/ganglia/gmond.conf
systemctl enable gmond
systemctl start gmond

# firewall settings
# all from local subnet - replace with yours
firewall-cmd --zone=public --add-rich-rule='rule family="ipv4" source address="f.g.h.i/j" accept'
# ssh, slurm, perfsonar should already be in zone public
# ganglia from central
firewall-cmd --zone=public --add-rich-rule='rule family="ipv4" source address="128.6.226.160/27" port port=8649 protocol=tcp accept'
firewall-cmd --runtime-to-permanent

# singularity
yum -y install singularity-ohpc



# complete <node1-2> (bare metal and mace keys already done)
# commands run from mace
for i in {1..2};do scp /etc/hosts <node>$i:/etc;done
for i in {1..2};do scp /etc/hosts.allow <node>$i:/etc;done
for i in {1..2};do scp /etc/hosts.deny <node>$i:/etc;done
pdsh -w <node>[1-2] ‘systemctl disable firewalld’

# ldap
pdsh -w <node>[1-2] 'yum -y install sssd openldap-clients'
pdsh -w <node>[1-2] ‘mkdir /etc/openldap/cacerts’
for i in {1..2};do echo <node>$i;scp ~/ern-poc/fedconfig/ern_hpc_rutgers_edu_interm.cer <node>$i:/etc/openldap/cacerts;ssh <node>$i ‘cacertdir_rehash /etc/openldap/cacerts/’;done
pdsh -w <node>[1-2] 'authconfig --updateall --enableldap --enableldapauth --ldapserver=ldap://ern.hpc.rutgers.edu:389 --ldapbasedn=dc=ufp,dc=hpc,dc=rutgers,dc=edu --enableldaptls --enableldapstarttls'
pdsh -w <node>[1-2] ‘systemctl restart sssd’
pdsh -w <node>[1-2] ‘id babbott’

# slurm
# should be 20003, 20002
pdsh -w <node>[1-2] ‘id slurm;id munge’
# might not need ohpc-slurm-client here, just server
pdsh -w <node>[1-2] ‘yum -y install ohpc-slurm-client ohpc-slurm-server’
for i in {1..2};do scp ~/fedconfig/munge.key <node>$i:/etc/munge/munge.key;done
pdsh -w <node>[1-2] ‘systemctl restart munge’
for i in {1..2};do scp ~/fedconfig/slurm.conf <node>$i:/etc/slurm;done
pdsh -w <node>[1-2] “systemctl enable slurmd;systemctl restart slurmd”

# nfs mounts - do on each compute node
ssh <node>1
# comment out local /home if present, add nfs home
#<scheduler.local>:/home /home                   nfs     defaults        0 0
vi /etc/fstab
exit
# repeat for <node>2
# back to <scheduler>
pdsh -w <node>[1-2] 'umount /home;mount /home'

# cephfs
# on <node>1-2
ssh <node>1
# add to fstab
#<scheduler.local>:/home /home        nfs    defaults    0 0
vi /etc/fstab
mount /home
exit

# ganglia
pdsh -w <node>[1-2] 'yum -y install ganglia-gmond-ohpc'
for i in {1..2};do echo <node>$i;scp /etc/ganglia/gmond.conf <node>$i:/etc/ganglia;done
pdsh -w <node>[1-2] 'systemctl enable gmond'
pdsh -w <node>[1-2] 'systemctl start gmond'

# confirm user can ssh from <scheduler> to <node1-2> without password
su - babbott
for i in {1..2};do ssh <node>$i “uptime”;done
exit

# singularity
pdsh -w <node>[1-2] 'yum -y install singularity-ohpc'
